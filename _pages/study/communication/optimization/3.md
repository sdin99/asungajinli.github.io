---
layout: posts
title: "Mathematical Methods for Wireless Communications"
permalink: /study/communication/optimization/3/
description: 제목부터가 Mathematical Methods, 무시무시하다. 하지만 겁먹을 필요는 없다. 하다보면 익숙해지고 재밌어 지기도 한다. 수학은 예로부터 과학과 기술의 언어로 사용되어왔다. 우리는 본격적인 최적화에 들어가기 앞서 필요한 여러가지 수학적 지식을 얻어야 한다. 이제 여기에서 무선 통신에 초점을 맞춘 수학적 배경과 셀룰러 시스템의 수학적 모델에 대해 알아갈 수 있을것이다.
---

# Signal Spaces

<img class="modal img__medium" src="/_pages/study/communication/optimization/images/3/1.png" alt="<b>[Fig. 1]</b> Shannon’s communication architecture <a href='#Reference'>[1]</a>."/>

위 그림은 우리가 흔히 알고 있는 통신 시스템의 구조를 나타낸 것이다. 이러한 통신 시스템을 수학적으로 분석하기 위한 가장 기본적인 개념은 **Signal Space** 이다.



# Approximation and Estimation in Signal Spaces


## Minimum Mean-Squared Error Estimation

추정이라는 단어가 본의 아니게 많이 사용되게 되는데 추정, 추정값, 추정자 등등 헷갈리는 사태를 방지하기 위해 추정자는 영어로 estimator로 그대로 사용한다.

MMSE 추정은 확률적 모델을 기반으로 한다. [Fig. 1]에서 볼 수 있듯 무선 통신 시스템에는 noise에 의한 불확실성이 존재한다. Noise 뿐만 아니라 간섭, 감쇠 등으로 인해 메시지가 손상되어 수신된다. 이러한 불확실성을 다루는데 매우 유용한 확률 이론을 사용한다. 확률 이론 자체에 대한 내용은 생략한다. 랜덤 변수의 특성, 가우시안 분포와 같은 내용이니 궁금하다면 확률 이론에 대한 책을 참고하길 바란다.

추정 이론은 관측에 기반하여 파라미터의 값을 추정한다. 추정에는 Bayesian(베이지안)과 Frequentist(빈도주의) 두 가지 접근 방법이 있다. Frequentist는 전통적인 접근 방식으로, 우리는 추정하려는 파라미터가 어떤 값인지 모르지만 그 값은 확정적이라고 가정한다. '빈도'라는 말에서 알 수 있듯이 빈도주의는 무한히 반복해서 발생하는 사건들을 관측하고 그로부터 도출되는 파라미터는 불변의 값으로 취급한다.

반대로 Bayesian 접근 방식은 'Bayesian formula'를 기반으로 하는데, 파라미터는 언제든지 변할 수 있다고 본다. $\text{(Posteriori)} = \frac{\text{(Likelihood)} \times \text{(Prior)}}{\text{(Occurrence)}}$ 에서 알 수 있듯 Posteriori(사후확률)는 Prior(사전확률)과 Likelihood(가능도)를 기반으로 결정된다. Prior는 데이터를 관측하기 전에 이미 알고 있는 정보, Likelihood는 파라미터에 대한 관측 데이터를 바탕으로 결정된다. 즉 Bayesian 접근 방식은 사전 정보와 관측 데이터에 대한 가능성 둘 다에 의존한다. Likelihood는 관측을 거듭할수록 업데이트 될 수 있으며, 마찬가지로 추정하고자 하는 파라미터 또한 업데이트 될 수 있다. 때문에 Bayesian은 Frequentist에 비해 계산 복잡도가 훨씬 높다. 컴퓨팅 성능이 좋아지고 데이터의 양이 많아지면서 Bayesian 추정 방법이 점점 많이 사용되고 있다.

Frequentist 방식에 익숙해져 있는 우리 입장에서는 Bayesian 추정이 잘 와닿지 않을 수 있다. 그럴 경우에는 '스팸 메일' 예시가 있는데 한번 찾아보길 추천한다. 이해하는데 어느정도 도움이 된다.
{:.notice}

파라미터를 확정적 변수로 추정하기 위해서는 least square estimator, maximum likelihood (ML) estimator, best linear unbiased estimator, minimum variance unbiased (MVU) estimator 등을 사용할 수 있으며 반대로 파라미터를 랜덤 변수로 간주한다면 MMSE estimator를 사용할 수 있다.

### MVU Estimator

먼저 MVU estimator에 대해 알아보자. 랜덤 변수 집합
$
X = \begin{bmatrix} X_1, \\ X_2, \\ \ldots \\ ,X_n \end{bmatrix}^\intercal
$
가 pdf
$
p(x | \theta)
$
에 따라 독립적으로 분포되어 있다고 가정한다. $\theta$는 미지의 파라미터다. $\theta$의 추정값을 $\hat{\theta}$라 정의하고 estimator 함수를 $g()$라 하면 $\hat{\theta}$는 아래와 같이 표현할 수 있다.

$$
\hat{\theta} = g(X_1, X_2, \ldots, X_n)
$$

이렇게 얻어진 추정값이 참 값에 얼마나 가까운지 알기 위해 Mean Square Error(MSE)를 다음과 같이 정의한다.

$$
\text{MSE}_{\hat{\theta}} = \mathbb{E}\{(\hat{\theta}-\theta)^2\}
$$

Estimator $\hat{\theta}$의 Bias는 다음과 같고, (모든 $\theta$에 대해 $\text{Bias}(\hat{\theta}) = 0$이면 $\hat{\theta}$를 unbiased estimator라고 한다.)

$$
\text{Bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta
$$

앞선 MSE 식을 쭉 풀어 쓰면 우리는 다음과 같은 결과를 얻게된다.

$$
\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \left( \text{Bias}(\hat{\theta}) \right)^2
$$

MVU estimator는 Bias를 0이라 두고 Variance를 최소화하는 것으로 정의된다.

$$
\begin{align*}
\hat{\theta}_{\text{MVU}} &= \arg\min\limits_{\hat{\theta}} (\text{Var}(\hat{\theta})) \\
\mathbb{E}(\hat{\theta}) &= \theta, \quad a < \theta < b
\end{align*}
$$

<div class="post__stage-container">
    <div class="post__stage">
        <img class="modal" src="/_pages/study/communication/optimization/images/3/2.webp" alt="<b>[Fig. 2]</b> Bias-variance tradeoff <a href='#Reference'>[3]</a>."/>
    </div>
    <div class="post__stage">
        <p>
            Variance와 Bias 개념을 쉽게 이해하려면 과녁판을 생각해 보면 된다. 우리가 얻고 싶은 모집단의 모수($\theta$)가 과녁판의 정중앙에 있다고 생각해보자.
        </p>
        <p>
            표본을 뽑아 추정된 추정값을 화살이라고 본다면 [Fig. 2]와 같이 화살히 박히게 될 것이다. 그리고 이 화살들의 평균을 Bias라고 볼 수 있다. Bias가 크다는 것은 화살들이 과녁판의 정중앙에서 멀리 떨어져 있다는 것을 의미한다. 그리고 화살들이 퍼져있는 정도는 Variance라고 볼 수 있다.
        </p>
        <p>
            여기서 또 중요한 점은, Bias와 Variance는 trade-off 관계에 있다는 것이다. Bias를 줄이면 Variance는 증가하고, Variance를 줄이면 Bias는 증가한다. 이를 bias-variance tradeoff라고 한다. Bias와 Variance의 적절한 trade-off를 찾는 것이 우리의 목표이다.
        </p>
        <p>
            그럼 어떻게 Bias와 Variance를 동시에 줄일 수 있을까? 이를 위해서는 더 많은 데이터를 사용하는 것이 좋다. 데이터의 양이 많아질수록 화살들이 과녁판의 정중앙에 더 가까이 붙게 될 것이다. 그리고 이를 통해 Bias와 Variance를 동시에 줄일 수 있게 된다.
        </p>
        <p>
            그런데 이러한 방법은 항상 가능한 것은 아니다. 데이터의 양이 무한하다고 가정하면 Bias와 Variance를 동시에 줄일 수 있지만, 데이터의 양이 유한하다면 Bias와 Variance 중 어느 하나는 줄일 수 없다. 따라서 우리는 적절한 trade-off를 찾아야 한다.
        </p>
    </div>
</div>
{:.notice--callout}

MVU estimator는 불편성(unbiasedness)을 가지지만, 그렇지 않은 경우도 있다. 이 경우 우리는 bias를 최소화하는 방법을 찾아야 한다. 이러한 추정 방법을 minimum bias unbiased(MBB) estimator라고 한다. MBB estimator는 MVU estimator보다 더 작은 bias를 가지지만, 그만큼 variance는 더 크다. 때문에 MBB estimator는 MVU estimator보다 더 큰 MSE를 가진다.

### MMSE Estimator

MVU estimator가 MSE를 최소화하는 추정 방법이었다면, MMSE estimator는 Bayesian mean squared error(BMSE)를 최소화하는 추정 방법이다. Bayesian 접근 방식에서는 사전 확률 밀도 함수 $p_{\theta}(\theta)$가 이미 주어져 있다.
BMSE는 다음과 같이 정의된다.

$$
\text{BMSE}_{\hat{\theta}} = \mathbb{E} \left( (\hat{\theta} - \theta)^2 \right) = \int \int (\hat{\theta} - \theta)^2 p_{x,\theta}(x, \theta) \, dx \, d\theta
$$

따라서 BMSE를 최소화하는 MMSE estimator는 다음과 같이 정의할 수 있다.

$$
\hat{\theta}_{\text{MMSE}} = \text{arg} \ \underset{\theta}{\text{min}} \left( \int (\hat{\theta} - \theta)^2 p_{\theta|x}(\theta|x) \, d\theta \right)
$$

<!-- 무선 통신 시스템에서 전송된 메시지가 랜덤 신호를 포함하고 수신된 메시지를 확실하게 예측할 수 없다고 가정한다. 우리는 랜덤 현상으로부터의 수치적 결과 또는 결과를 수치로 매핑하는 측정 가능한 함수로써 **랜덤 변수 $X$**를 정의할 수 있고, $X$는 랜덤 벡터로 확장가능하다. $X$의 누적 분포 함수(cdf) $F_X(x)$는 다음과 같이 정의할 수 있다. 그러면 무작위 변수

$$
F_X(x) = P(X \leq x), \quad -\infty < x < \infty
$$ -->






## Maximum Likelihood and Maximum A Posteriori Estimation




---

# <a name="Reference"></a>Reference

1. C. E. Shannon, “A mathematical theory of communication,” Bell Syst. Tech. J., vol. 27, pp. 379–423, 1948.
2. T. Bayes, "An Essay towards solving a Problem in the Doctrine of Chances. By the late Rev. Mr. Bayes, communicated by Mr. Price, in a letter to John Canton, M. A. and F. R. S.," Philosophical Transactions (1683-1775), Vol. 53 (1763), pp. 370-418 (49 pages), 1763.
3. Ivan Reznikov, "Stop Using The Same Image in Bias-Variance Trade-off Explanation," <i>Medium</i>, 2021. [Online]. Available: [https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54](https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54){:target="_blank"}. [Accessed: 16- Feb- 2024].
{:.post__reference}